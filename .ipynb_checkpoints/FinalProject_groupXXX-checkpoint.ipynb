{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoe recommender\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Jay Buensuceso\n",
    "- Mabel Szeto\n",
    "- Joshua Lapidario\n",
    "- Sialoi Taa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "\n",
    "In this paper, we explore the applications of heirarchical clustering when it relates to a recommendation system for shoes. Utilizing a modified Zappos50k dataset, we attempt to cluster each shoe into its appropriate shoe category as well as retrieve the top 3 most similar shoes in the dataset. \n",
    "\n",
    "To score our implemented clusters, we utilizes sillhouette scores to determine the efficacy of our clusters, ranking from -1 to 1 the effectiveness of our clusters.\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "At the core, this project explores the many capabilities of machine learning and heirarchical clustering. Fashion styles, more specifically with regards to shoes in this context, are popular ways for people to express themselves in modern society. With AI being the hot topic being discussed as well, it should come to no surprise that it can be applied to the context of fashion. Visual search for specific items of clothing and product recommendation systems have been developed for the market in recent years. There are a plethora of examples of this application in academia as well <a name=\"zakaryan\"></a>[<sup>[1]</sup>](#zakaryannote).\n",
    "\n",
    "There are a plethora of examples of this application in both industry and academia. In one instance, Modista, a computer vision and machine learning based retail recommendation service, relies on the three features of shape, texture, and color to provide recommendations for similar shoes. In a study by Borras et al. through a UAB Bellaterra research paper “High-Level Clothes Description based on Colour-Texture and Structural Features”, the authors interpreted five clothing combinations in a graphical manner to try to deduce what someone was wearing in an inputted image. Both of these cases yield interesting insight into machine learning for fashion applications, but are fairly simple in nature <a name=\"khosla\"></a>[<sup>[2]</sup>](#khoslanote).\n",
    "\n",
    "More complex application involve the use of Convolution Neural Networks (CNNs), one such example comes from a Stanford research paper by Khosla and Venkataraman entitled “Building Image-Based Shoe Search Using Convolutional Neural Networks”.[1] CNNs are a popular form of neural networks used in image classification by using the mathematical property of convolution to simplify computations during model training. In this paper, Khosla et. al. struggled in regards to the nature of retrieval being inherently subjective in comparison to the classification (because there is no concrete definition for a shoe to be “similar” to another shoe). In the end, the authors found that they were able to improve over traditional computer vision and machine learning techniques through the use of their deep learning CNN architectures <a name=\"khosla\"></a>[<sup>[2]</sup>](#khoslanote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are making a project to help one identify a pair of shoes but don’t know the name of them and want to know a recommended set of shoes. The algorithm will take pictures of shoes as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We used data from the UT Zappos50K dataset, a 291MB archive composed of images scrapped from Zappos.com by a research group at the University of Texas. https://vision.cs.utexas.edu/projects/finegrained/utzap50k/\n",
    "\n",
    "The dataset had consistent image sizes of 136 by 102 pixels, all images were in color, had clean white backgrounds, and minimal variation in shoe orientation. From the Shoes category we uploaded photos from the Sneakers and Athletic Shoes functional type folder, we did not use all the brands and shoe pictures in the brand because a visual inspection showed that “Athletic Shoes” varied from roller skates, hiking boots, to soccer cleats, whereas we only wanted sneakers for street wear. There were some issues with image compression as some shoes did not keep their original ratio so we avoided using those photos as well, this typically happened by brand so we avoided those brands. We uploaded a total of 2182 sneaker images for our training set, separated into folders by brand. For our test set we uploaded 20 sneaker images also from the UT Zappos50K dataset from differing brands. We chose to not use the metadata as we are making a primarily unsupervised project and the metadata was functionally expired as searching the current Zappos website not all the shoes are being sold today and another item on the website will come up instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Sillhouette Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Dimensionality Reduction with PCA\n",
    "\n",
    "After performing PCA, we obtained the eigenvalues. These represent the amount of variance explained by each principal component. We then obtained the cumulative explained variance for each PC, also known as the sum of explained variances for that component. For further analysis, we were able to plot the explained variance against the number of principal components. This is called a scree plot, and by examining it we were able to see where the explained variance started to level off. Visually, this is the point in the graph that represents an elbow. After this point, adding more principal components provides diminishing returns when trying to explain additional variance within the data. Ideally this is the point that will give us the optimal number of principal components to use.\n",
    "\n",
    "At 80 principal components, it can explain over 80% of the variance in the dataset. At 200 principal components, it only explains 91% of the variance. Thus, we decided on staying at 80 principal components due to the diminishing returns.\n",
    "\n",
    "![image](imgs/eigenvalues_from_pca.png)\n",
    "\n",
    "### Reuslts from Heirarchical Clustering of 80 Principal Components vs 200 Principal Components\n",
    "\n",
    "![image](imgs/80_principal_components.png)\n",
    "![image](imgs/200_princial_components.png)\n",
    "\n",
    "### Subsection 3\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "For the Zappos50K dataset, there is a limitation for the shoe recommendation as the dataset relies on the website being updated to the current site's CID system (the system used for the product's unique ID on zappos.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "As with any computer vision and machine learning problem, we need to be considerate with the consent and privacy of any individuals appearing within or affiliated with the dataset. We will ensure that the data we use for our project does not infringe upon anyone’s privacy or personal information, such as excluding faces and any indicators of personal addresses or locations from images we obtain. We will not be scraping data from sites that have a paywall, require password login to view, or any sites that restrict this type of data collection in their terms of service. The images and data that we will be scraping for are publicly accessible on the internet. We will be scraping only for the initial data collection part of our project to minimize load on the website's servers and spread the volume that we scrape across different sites to avoid disrupting the service for other users. We are not making generative AI that creates new images from the images in our datasets.\n",
    "\n",
    "A potential bias to consider would arise from an uneven distribution of categorized shoes, which may result in certain shoes and shoe styles having inaccurate predictions or irrelevant recommendations. This of course, would make our model less reliable and discourage people with diverse styles from using it. Thus, it will be important to utilize an evenly distributed and representative dataset for this project. Because of our limited time to execute on this project we will likely be scoping our project down to a specific style of shoe to test the validity of our model and include this caveat in our readme and final project submission. In a similar vein, we will not be able to create a search algorithm that can scrape for prices off of every website and social media listing online so our resulting recommendations will be from larger resale or retail sites that consolidate many shoe types and prices in the same place, further biasing our results. The resulting reported prices of shoes are not to be taken as true financial advice as this is not a final product for market but rather an academic project to learn how to apply machine learning techniques.\n",
    "\n",
    "Careful consideration through precise exploratory data analysis will help us cater to these potential biases and also expose any new biases that arise during the span of our project.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"zakaryannote\"></a>1.[^](#zakaryan):  Zakaryan, Vahan. “AI Clothing Detection: Use Cases for Fashion and E-Commerce.” *Postindustria*, 17 June 2022, https://postindustria.com/ai-clothing-detection-use-cases-for-fashion-and-e-commerce/\n",
    "<br>\n",
    "\n",
    "<a name=\"khoslanote\"></a>2.[^](#khosla): Khosla, Neal and Venkataraman Vignesh. “Building Image-Based Shoe Search Using Convolutional Neural Networks\" *Cs231n.Stanford.Edu,* 2015, http://cs231n.stanford.edu/reports/2015/pdfs/nealk_final_report.pdf\n",
    "<br> \n",
    "\n",
    "<a name=\"ayuanote\"></a>3.[^](#ayu): A. Yu and K. Grauman. \"Fine-Grained Visual Comparisons with Local Learning\". In CVPR, 2014, https://vision.cs.utexas.edu/projects/finegrained/utzap50k/\n",
    "<br> \n",
    "\n",
    "<a name=\"kgraumannote\"></a>4.[^](#kgrauman): A. Yu and K. Grauman. \"Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images\". In ICCV, 2017, https://vision.cs.utexas.edu/projects/finegrained/utzap50k/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
