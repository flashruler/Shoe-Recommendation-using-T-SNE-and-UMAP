{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjPLpCuZqE05xqtSVJsCxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabeltree/Girish-Fan-Club/blob/main/CNN_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepared data based on: https://www.kaggle.com/code/androbomb/using-cnn-to-classify-images-w-pytorch\n",
        "\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "4C501FM7OZ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mabeltree/Girish-Fan-Club.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzeJPRgj-ICF",
        "outputId": "cfbefafd-c488-4eab-e5b1-f803cecc1ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Girish-Fan-Club'...\n",
            "remote: Enumerating objects: 2599, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 2599 (delta 6), reused 126 (delta 3), pack-reused 2469\u001b[K\n",
            "Receiving objects: 100% (2599/2599), 13.90 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image as mp_image\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "AAzS-6f2B0gu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The images are in a folder named 'input/natural-images/natural_images'\n",
        "training_folder_name = '../content/Girish-Fan-Club/sneakers'\n",
        "\n",
        "# All images are 128x128 pixels\n",
        "img_size = (128,128)\n",
        "\n",
        "# The folder contains a subfolder for each class of shape\n",
        "classes = sorted(os.listdir(training_folder_name))\n",
        "print(classes)\n"
      ],
      "metadata": {
        "id": "6fD-XqlU4F4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899fc334-107c-4159-c3d2-840c6f03e1a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ALDO', 'AdidasOriginals', 'Aetrex', 'AlexanderMcQueen', 'Amiana', 'BetseyJohnson', 'Converse', 'Crocs', 'DSQUARED2', 'DVS Shoe Company', 'ECCO', 'Easy Spirit', 'GUESS', 'KangaROOS', 'Lacoste', 'Nike', 'Paul Smith', 'Reebok', 'SKECHERS', 'UGG', 'adidasY-3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "N630m6qvCKR8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# function to resize image\n",
        "def resize_image(src_image, size=(128,128), bg_color=\"white\"):\n",
        "    from PIL import Image, ImageOps\n",
        "\n",
        "    # resize the image so the longest dimension matches our target size\n",
        "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "    # Create a new square background image\n",
        "    new_image = Image.new(\"RGB\", size, bg_color)\n",
        "\n",
        "    # Paste the resized image into the center of the square background\n",
        "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
        "\n",
        "    # return the resized image\n",
        "    return new_image"
      ],
      "metadata": {
        "id": "OrM3axwnChpP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# New location for the resized images\n",
        "train_folder = '../working/data/natural_images'\n",
        "\n",
        "\n",
        "# Create resized copies of all of the source images\n",
        "size = (128,128)\n",
        "\n",
        "# Create the output folder if it doesn't already exist\n",
        "if os.path.exists(train_folder):\n",
        "    shutil.rmtree(train_folder)\n",
        "\n",
        "# Loop through each subfolder in the input folder\n",
        "print('Transforming images...')\n",
        "for root, folders, files in os.walk(training_folder_name):\n",
        "    for sub_folder in folders:\n",
        "        print('processing folder ' + sub_folder)\n",
        "        # Create a matching subfolder in the output dir\n",
        "        saveFolder = os.path.join(train_folder,sub_folder)\n",
        "        if not os.path.exists(saveFolder):\n",
        "            os.makedirs(saveFolder)\n",
        "        # Loop through the files in the subfolder\n",
        "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
        "        for file_name in file_names:\n",
        "            # Open the file\n",
        "            file_path = os.path.join(root,sub_folder, file_name)\n",
        "            #print(\"reading \" + file_path)\n",
        "            image = Image.open(file_path)\n",
        "            # Create a resized version and save it\n",
        "            resized_image = resize_image(image, size)\n",
        "            saveAs = os.path.join(saveFolder, file_name)\n",
        "            #print(\"writing \" + saveAs)\n",
        "            resized_image.save(saveAs)\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w968EQunDBm5",
        "outputId": "be13791a-c3a2-4539-fb13-ff872c8e5166"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transforming images...\n",
            "processing folder UGG\n",
            "processing folder DSQUARED2\n",
            "processing folder DVS Shoe Company\n",
            "processing folder Nike\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d6bfecb2a2da>:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  src_image.thumbnail(size, Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing folder ALDO\n",
            "processing folder Reebok\n",
            "processing folder Lacoste\n",
            "processing folder Paul Smith\n",
            "processing folder Aetrex\n",
            "processing folder Crocs\n",
            "processing folder AdidasOriginals\n",
            "processing folder BetseyJohnson\n",
            "processing folder KangaROOS\n",
            "processing folder AlexanderMcQueen\n",
            "processing folder Amiana\n",
            "processing folder ECCO\n",
            "processing folder SKECHERS\n",
            "processing folder Easy Spirit\n",
            "processing folder GUESS\n",
            "processing folder adidasY-3\n",
            "processing folder Converse\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_path):\n",
        "    import torch\n",
        "    import torchvision\n",
        "    import torchvision.transforms as transforms\n",
        "    # Load all the images\n",
        "    transformation = transforms.Compose([\n",
        "        # Randomly augment the image data\n",
        "            # Random horizontal flip\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "            # Random vertical flip\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        # transform to tensors\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "\n",
        "\n",
        "    # Split into training (70% and testing (30%) datasets)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "\n",
        "    # use torch.utils.data.random_split for training/test split\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    # define a loader for the training data we can iterate through in 50-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # define a loader for the testing data we can iterate through in 50-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Recall that we have resized the images and saved them into\n",
        "train_folder = '../working/data/natural_images'\n",
        "\n",
        "# Get the iterative dataloaders for test and training data\n",
        "train_loader, test_loader = load_dataset(train_folder)\n",
        "batch_size = train_loader.batch_size\n",
        "print(\"Data loaders ready to read\", train_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_o00jr3DcwV",
        "outputId": "35e44378-7681-4693-e150-eafb3faab32a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaders ready to read ../working/data/natural_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "\n",
        "    # Defining the Constructor\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...\n",
        "\n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = Net(num_classes=len(classes)).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "IktqxLkREUyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}